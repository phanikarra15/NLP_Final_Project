{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "fSLKZ2lcJx06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mithi\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: requests in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.16.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.19.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.5->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the transformers for using pre-trained Models\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "s6KNiijNYN-K"
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "import torch\n",
    "import pickle\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from earlystopping import EarlyStopping\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "RcGSFaiTklFM"
   },
   "outputs": [],
   "source": [
    "#Defining the tokenizer and pre_trained model \n",
    "#Incase of ERNIE Large Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "pre_trained_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')\n",
    "\n",
    "#Incase of ERNIE Normal Model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
    "#pre_trained_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "rpetzFRcWBmg"
   },
   "outputs": [],
   "source": [
    "# Load the train, test and dev dataset\n",
    "# Done by Phani\n",
    "def load_dataset(filename):\n",
    "    with open(filename,'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "qwPuzPYsWBp3"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos tags, probablities in a single list from both the Train and Dev dataset\n",
    "# Done by Phani\n",
    "def word_traindev_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    probabilities = []\n",
    "    wordList = []\n",
    "    pos = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]\n",
    "            prob = lineSplit[4]\n",
    "            temp = lineSplit[5]\n",
    "            words.append(word)\n",
    "            probabilities.append(float(prob))\n",
    "            pos.append(temp)\n",
    "        elif not (len(empty) and []):\n",
    "            wordList.append((words, pos, probabilities))\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            pos = []\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "1ugzYtznWB1X"
   },
   "outputs": [],
   "source": [
    "# Getting the words in a single list from the Test dataset\n",
    "# Done by Phani\n",
    "def word_test_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    testWord = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]            \n",
    "            words.append(word)\n",
    "        elif not len(empty):\n",
    "            testWord.append(words)\n",
    "            words = []       \n",
    "    return testWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "hpkAMX2uWB3y"
   },
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Train and Dev data\n",
    "# Done by Phani\n",
    "def data_preprocess_train_dev(data):\n",
    "    text = []\n",
    "    pos = []\n",
    "    probs = []\n",
    "    for i,j,k in data:\n",
    "            text.append(i)\n",
    "            pos.append(j)\n",
    "            probs.append(k)\n",
    "    return text,pos, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "3VdGfMGMklFY"
   },
   "outputs": [],
   "source": [
    "# Generate separate list of words for Test data\n",
    "# Done by Phani\n",
    "def data_preprocess_test(data):\n",
    "    text = []\n",
    "    for i in data:\n",
    "            text.append(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "iJVUFOOGklFa"
   },
   "outputs": [],
   "source": [
    "# Replicating probablities for matching length incase of sub tokenized words\n",
    "#Done by Mithilaesh\n",
    "def prob_list(batch_data,batch_probs):\n",
    "    pb = []\n",
    "    for i,j in zip(batch_data,batch_probs):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(float(l))\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(float(l))\n",
    "        pb.append(tp)\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "M7EHKVPOklFc"
   },
   "outputs": [],
   "source": [
    "# Replicating feature vectors for matching length incase of sub tokenized words\n",
    "#Done by Mithilaesh\n",
    "def feature_list(batch_data,feature):\n",
    "    fv = []\n",
    "    for i,j in zip(batch_data,feature):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(l)\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(l)\n",
    "        fv.append(tp)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "wpTducdXklFf"
   },
   "outputs": [],
   "source": [
    "# Generate sentence from words in dataset\n",
    "#Done by Mithilaesh\n",
    "def get_sentence(words):    \n",
    "    tokenized_text = []\n",
    "    for i in words:\n",
    "        sent = ''\n",
    "        for h in i:\n",
    "            if sent == '':\n",
    "                sent = sent + h\n",
    "            else:\n",
    "                sent = sent+ \" \" +h\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tid = tokenizer.encode(tokens, add_special_tokens=False)\n",
    "        tokenized_text.append(tid)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "YJ1nDMgkklFh"
   },
   "outputs": [],
   "source": [
    "# function to pad data for equal length\n",
    "#Done by Mithilaesh\n",
    "def pad_func(data):\n",
    "    max_len = 0\n",
    "    for i in data:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    if type(i[0]) is list:\n",
    "        padded = [i + [[0, 0, 0, 0, 0]]*(max_len-len(i)) for i in data]\n",
    "    else:\n",
    "        padded = [i + [0]*(max_len-len(i)) for i in data]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "wr--cGRVklFk"
   },
   "outputs": [],
   "source": [
    "#data augmentation function to randomly reverse a sentence, capitalize a word and remove a word from a sentence\n",
    "#Done by Mithilaesh\n",
    "def data_augment(words, probs):\n",
    "    aug_word_list = []\n",
    "    aug_prob_list = []\n",
    "    for i in range(len(words)):\n",
    "        aug_word_list.append(words[i])\n",
    "        aug_prob_list.append(probs[i])\n",
    "        \n",
    "        if (i%2) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word.reverse()\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            temp_prb.reverse()\n",
    "            aug_prob_list.append(temp_prb)\n",
    "            \n",
    "        if (i%3) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word[0] = temp_word[0].upper()\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            aug_prob_list.append(temp_prb)\n",
    "            \n",
    "        if (i%5) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word.remove(temp_word[0])\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            temp_prb.remove(temp_prb[0])\n",
    "            aug_prob_list.append(temp_prb)\n",
    "                \n",
    "    return aug_word_list, aug_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Ju9NV_ayklFm"
   },
   "outputs": [],
   "source": [
    "#create feature vector for the words based on starts with capital, full word is capital, has hashtags, \n",
    "#word can be tokenized and word that is a connector word\n",
    "#Done Srihasa\n",
    "def feature_add(trainWords, conn_words):\n",
    "    feature = []\n",
    "    for i in trainWords:\n",
    "        temp1 = []\n",
    "        for j in i:\n",
    "            temp2 =[0] * 5\n",
    "            if (j[0].isupper()) and (j not in conn_words):\n",
    "                temp2[0] = 1\n",
    "            else:\n",
    "                temp2[0] = 0\n",
    "            if '#' in j:\n",
    "                temp2[1] = 1\n",
    "            else:\n",
    "                temp2[1] = 0\n",
    "            if j.isupper() and (j not in conn_words):\n",
    "                temp2[2] = 1\n",
    "            else:\n",
    "                temp2[2] = 0\n",
    "            if (len(tokenizer.tokenize(j))>1) and (j not in conn_words):\n",
    "                temp2[3] = 1\n",
    "            else:\n",
    "                temp2[3] = 0\n",
    "            if j not in conn_words:\n",
    "                temp2[4] = 1\n",
    "            else:\n",
    "                temp2[4] = 0\n",
    "            temp1.append(temp2)\n",
    "        feature.append(temp1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "HE0AVpnkklFo"
   },
   "outputs": [],
   "source": [
    "#get a list of connector words from the dataset\n",
    "#Done Srihasa\n",
    "def connectors(trainWords, trainLabels):\n",
    "    words = []\n",
    "    for i,j in zip(trainWords, trainLabels):\n",
    "        for k,l in zip(i,j):\n",
    "            if l == 0.0:\n",
    "                words.append(k)\n",
    "    mylist = list(dict.fromkeys(words))\n",
    "    mylist.sort()\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "nMi1gSVNklFq"
   },
   "outputs": [],
   "source": [
    "#function to shuffle the dataset \n",
    "#Done Srihasa\n",
    "def func_shuffle(tokens, probablities, feature):\n",
    "    mapIndexPosition = list(zip(tokens, probablities, feature))\n",
    "    np.random.shuffle(mapIndexPosition)\n",
    "    tokens, probablities, feature = zip(*mapIndexPosition)\n",
    "    return tokens, probablities, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "rXhkTjUfklFt"
   },
   "outputs": [],
   "source": [
    "# function to get attention mask\n",
    "#Done Srihasa\n",
    "def gen_attention(data):\n",
    "    attention_mask = []\n",
    "    for i in data:\n",
    "        tmp = list([1] * (np.count_nonzero(i))) + list([0] * (len(i) - (np.count_nonzero(i))))\n",
    "        attention_mask.append(tmp)\n",
    "    return attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "6n7FCyJQWB7K"
   },
   "outputs": [],
   "source": [
    "# Specifying dataset file names\n",
    "#Done by Phani\n",
    "TRAINING_FILE = \"train.txt\"\n",
    "DEV_FILE = \"dev.txt\"\n",
    "TEST_FILE = \"test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "uBpfUuPwYHgf"
   },
   "outputs": [],
   "source": [
    "# Preprocessing work on the dataset \n",
    "#Done by Phani\n",
    "trainText = word_traindev_Data(load_dataset(TRAINING_FILE))\n",
    "testEval = word_test_Data(load_dataset(TEST_FILE))\n",
    "devText = word_traindev_Data(load_dataset(DEV_FILE))\n",
    "\n",
    "trainWords,trainTags, trainLabels = data_preprocess_train_dev(trainText)\n",
    "devWords, devTags, devLabels = data_preprocess_train_dev(devText)\n",
    "testWords = data_preprocess_test(testEval)\n",
    "\n",
    "#augmenting the dataset size\n",
    "#trainWords, trainLabels = data_augment(trainWords, trainLabels)\n",
    "#devWords, devLabels = data_augment(devWords, devLabels)\n",
    "\n",
    "#getting connectors from dataset\n",
    "conn_words = connectors(trainWords, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "aS0vsmULklFz"
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "#Done by Phani\n",
    "train_tokens = get_sentence(trainWords)\n",
    "train_probablities = prob_list(trainWords,trainLabels)\n",
    "train_features = feature_add(trainWords, conn_words)\n",
    "train_feature = feature_list(trainWords,train_features)\n",
    "\n",
    "train_tokens, train_probablities, train_feature = func_shuffle(train_tokens, train_probablities, train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "9QgSScClklF1"
   },
   "outputs": [],
   "source": [
    "#Dev data\n",
    "#Done by Phani\n",
    "dev_tokens = get_sentence(devWords)\n",
    "dev_probablities = prob_list(devWords,devLabels)\n",
    "dev_features = feature_add(devWords,conn_words)\n",
    "dev_feature = feature_list(devWords,dev_features)\n",
    "\n",
    "dev_tokens, dev_probablities, dev_feature = func_shuffle(dev_tokens, dev_probablities, dev_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "xa8Kf3q2rgdx"
   },
   "outputs": [],
   "source": [
    "#defining the model class\n",
    "#Done by Mithilaesh\n",
    "class ErnieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErnieModel, self).__init__()\n",
    "        self.ernie = pre_trained_model\n",
    "        #self.linear = nn.Linear(773, 1) Incase of ERNIE Normal \n",
    "        self.linear = nn.Linear(1029, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, tokens, attention, feature_vect):\n",
    "        pooled_output,_ = self.ernie(tokens, attention)\n",
    "        final_op = torch.cat((pooled_output, feature_vect), dim=-1)\n",
    "        linear_output = self.linear(final_op)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "j5aipVryklF5"
   },
   "outputs": [],
   "source": [
    "#setting model parameters\n",
    "#Done by Mithilaesh\n",
    "model = ErnieModel()\n",
    "\n",
    "model_path = 'ernie_1024_layer_lr0001.pth'\n",
    "#model_path = 'ernie_768_layer_lr0001.pth'\n",
    "#model_path = 'ernie_768_layer_lr01.pth'\n",
    "\n",
    "early_stopping = EarlyStopping(model_path,4,True)\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.MSELoss(reduction = 'mean')\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "B70NVVPoepQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch number ---->0\n",
      "Training loss ---->0.1910816727365766\n",
      "Total runtime ----> 1718.4136307239532 seconds\n",
      "\n",
      "Validation loss ---->0.19444727659225464\n",
      "Total runtime ----> 65.90989923477173 seconds\n",
      "\n",
      "Validation loss is (inf --> 0.19445).  Saving model ...\n",
      "Running epoch number ---->1\n",
      "Training loss ---->0.19243688140596663\n",
      "Total runtime ----> 1703.3324127197266 seconds\n",
      "\n",
      "Validation loss ---->0.19446736991405486\n",
      "Total runtime ----> 65.60792970657349 seconds\n",
      "\n",
      "Early Stopping = 1 of 4\n",
      "Running epoch number ---->2\n",
      "Training loss ---->0.19100528700011118\n",
      "Total runtime ----> 1688.3708982467651 seconds\n",
      "\n",
      "Validation loss ---->0.19418314337730408\n",
      "Total runtime ----> 65.87995219230652 seconds\n",
      "\n",
      "Validation loss is (0.19445 --> 0.19418).  Saving model ...\n",
      "Running epoch number ---->3\n",
      "Training loss ---->0.1919568237236568\n",
      "Total runtime ----> 1704.4216659069061 seconds\n",
      "\n",
      "Validation loss ---->0.1966002243757248\n",
      "Total runtime ----> 67.31215262413025 seconds\n",
      "\n",
      "Early Stopping = 1 of 4\n",
      "Running epoch number ---->4\n",
      "Training loss ---->0.19240722383771625\n",
      "Total runtime ----> 1667.5083048343658 seconds\n",
      "\n",
      "Validation loss ---->0.195289204120636\n",
      "Total runtime ----> 67.29924273490906 seconds\n",
      "\n",
      "Early Stopping = 2 of 4\n",
      "Running epoch number ---->5\n",
      "Training loss ---->0.1914299852507455\n",
      "Total runtime ----> 1674.5014901161194 seconds\n",
      "\n",
      "Validation loss ---->0.19288968026638031\n",
      "Total runtime ----> 66.74867987632751 seconds\n",
      "\n",
      "Validation loss is (0.19418 --> 0.19289).  Saving model ...\n",
      "Running epoch number ---->6\n",
      "Training loss ---->0.19234607645443508\n",
      "Total runtime ----> 1835.0241911411285 seconds\n",
      "\n",
      "Validation loss ---->0.19252134382724762\n",
      "Total runtime ----> 75.75308275222778 seconds\n",
      "\n",
      "Validation loss is (0.19289 --> 0.19252).  Saving model ...\n",
      "Running epoch number ---->7\n",
      "Training loss ---->0.19154869181769235\n",
      "Total runtime ----> 1860.0743517875671 seconds\n",
      "\n",
      "Validation loss ---->0.19552397668361665\n",
      "Total runtime ----> 70.39133739471436 seconds\n",
      "\n",
      "Early Stopping = 1 of 4\n",
      "Running epoch number ---->8\n",
      "Training loss ---->0.19191059367997307\n",
      "Total runtime ----> 1947.8655948638916 seconds\n",
      "\n",
      "Validation loss ---->0.1943608909845352\n",
      "Total runtime ----> 78.42025518417358 seconds\n",
      "\n",
      "Early Stopping = 2 of 4\n",
      "Running epoch number ---->9\n",
      "Training loss ---->0.19260537334850855\n",
      "Total runtime ----> 1879.471114873886 seconds\n",
      "\n",
      "Validation loss ---->0.19731252193450927\n",
      "Total runtime ----> 67.61200475692749 seconds\n",
      "\n",
      "Early Stopping = 3 of 4\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "#Done by Mithilaesh\n",
    "for epoch_num in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    print(\"Running epoch number ---->{}\".format(epoch_num))\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(0, len(train_tokens), batch):\n",
    "        model.zero_grad()\n",
    "        train_tokens_pad, train_probablities_pad, train_feature_pad = func_shuffle(train_tokens[i:i+batch], train_probablities[i:i+batch], train_feature[i:i+batch])\n",
    "        train_tokens_pad = pad_func(train_tokens_pad)\n",
    "        train_probablities_pad = pad_func(train_probablities_pad)\n",
    "        train_feature_pad = pad_func(train_feature_pad)\n",
    "        train_attention_pad = gen_attention(train_tokens_pad)\n",
    "        train_probas = model(torch.tensor(train_tokens_pad), torch.tensor(train_attention_pad), torch.tensor(train_feature_pad))\n",
    "        train_grd_truth = []\n",
    "        for i in train_probablities_pad:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            train_grd_truth.append(p)\n",
    "        train_batch_loss = loss_func(train_probas, torch.tensor(train_grd_truth))\n",
    "        training_loss.append(train_batch_loss.item())\n",
    "        train_batch_loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Training loss ---->{}\".format((np.average(training_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    \n",
    "    #Validation Run\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for i in range(0, len(dev_tokens), batch):\n",
    "            dev_tokens_pad, dev_probablities_pad, dev_feature_pad = func_shuffle(dev_tokens[i:i+batch], dev_probablities[i:i+batch], dev_feature[i:i+batch])\n",
    "            dev_tokens_pad = pad_func(dev_tokens_pad)\n",
    "            dev_probablities_pad = pad_func(dev_probablities_pad)\n",
    "            dev_feature_pad = pad_func(dev_feature_pad)\n",
    "            dev_attention_pad = gen_attention(dev_tokens_pad)\n",
    "            dev_probas = model(torch.tensor(dev_tokens_pad), torch.tensor(dev_attention_pad), torch.tensor(dev_feature_pad))\n",
    "            dev_grd_truth = []\n",
    "            for i in dev_probablities_pad:\n",
    "                p = []\n",
    "                for j in i:\n",
    "                    q=[]\n",
    "                    q.append(j)\n",
    "                    p.append(q)\n",
    "                dev_grd_truth.append(p)\n",
    "            dev_batch_loss = loss_func(dev_probas, torch.tensor(dev_grd_truth))\n",
    "            validation_loss.append(dev_batch_loss.item())\n",
    "        \n",
    "    print(\"Validation loss ---->{}\".format((np.average(validation_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    early_stopping(np.average(validation_loss), model)\n",
    "\n",
    "    if early_stopping.early_stop is True:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "5HL0KYS9klGA"
   },
   "outputs": [],
   "source": [
    "#loading the trained model\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "hI_i3MI_klGC"
   },
   "outputs": [],
   "source": [
    "#Test Data\n",
    "#Done by Srihasa\n",
    "tokenized_test_text = []\n",
    "for i in testWords:\n",
    "    sent = \"\"\n",
    "    for j in i:\n",
    "        if sent == \"\":\n",
    "            sent += j\n",
    "        else:\n",
    "            sent = sent + \" \" + j\n",
    "    tokenized_test_text.append(sent)\n",
    "test_features = feature_add(testWords, conn_words)\n",
    "test_feature = feature_list(testWords,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "e-nRQa1lklGE"
   },
   "outputs": [],
   "source": [
    "# Testing the model on Test Dataset\n",
    "#Done by Srihasa\n",
    "test_prob=[]\n",
    "test_tokens = []\n",
    "test_res = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, feat in zip(tokenized_test_text, test_feature):\n",
    "        cp = []\n",
    "        tokens = tokenizer.tokenize(batch_data)\n",
    "        test_tokens.append(tokens)\n",
    "        tid = tokenizer.encode_plus(tokens, add_special_tokens=False, return_attention_mask=True, return_tensors='pt')\n",
    "        cp.append(feat)\n",
    "        test_probas = model(tid['input_ids'], tid['attention_mask'], torch.tensor(cp))\n",
    "        test_res.append(test_probas.data.tolist())\n",
    "        out = batch_data.split(\" \")\n",
    "        temp_ans = []\n",
    "        index = 0\n",
    "        for i in out:\n",
    "            if (len(tokenizer.tokenize(i))) == 1:\n",
    "                temp_ans.append(test_probas[0][index].item())\n",
    "                index = index + 1\n",
    "            else:\n",
    "                holder = []\n",
    "                for j in range(len(tokenizer.tokenize(i))):\n",
    "                    holder.append(test_probas[0][index].item())\n",
    "                    index = index + 1\n",
    "                prb = np.average(holder)\n",
    "                temp_ans.append(prb) \n",
    "        test_prob.append(temp_ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ZMjNdTVEklGG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = We 'll be closed from 12/24 to 1/1 . See you in the New Year !\n",
      "Emphasis Values = [0.43861860036849976, 0.44173575937747955, 0.4431616961956024, 0.43971166014671326, 0.4531821012496948, 0.4483940601348877, 0.41523823142051697, 0.4402421514193217, 0.4406854510307312, 0.46668708324432373, 0.42807328701019287, 0.4387187659740448, 0.4422082006931305, 0.44605904817581177, 0.44864422082901, 0.48147207498550415]\n",
      "\n",
      "Sentence = No matter how hard you work , someone else is working harder .\n",
      "Emphasis Values = [0.4177796244621277, 0.454275518655777, 0.4609808623790741, 0.4430331587791443, 0.3878982365131378, 0.4430483281612396, 0.3943283259868622, 0.39992910623550415, 0.39056774973869324, 0.4118628203868866, 0.43296104669570923, 0.44407224655151367, 0.466374009847641]\n",
      "\n",
      "Sentence = The less I needed , the better I felt .\n",
      "Emphasis Values = [0.4869804382324219, 0.4960853159427643, 0.4346615970134735, 0.41988831758499146, 0.43193504214286804, 0.4629516899585724, 0.49009206891059875, 0.41067948937416077, 0.4914662837982178, 0.4547588527202606]\n",
      "\n",
      "Sentence = When someone shows you who they are , believe them the first time .\n",
      "Emphasis Values = [0.44292840361595154, 0.44265812635421753, 0.39894968271255493, 0.4007332921028137, 0.4598458409309387, 0.45089513063430786, 0.44148918986320496, 0.4387178421020508, 0.48124054074287415, 0.4303879141807556, 0.4518088400363922, 0.42933955788612366, 0.4520246386528015, 0.4595211446285248]\n",
      "\n",
      "Sentence = You are not a drop in the ocean . You are the entire ocean in a drop .\n",
      "Emphasis Values = [0.4945492148399353, 0.44652318954467773, 0.4595990478992462, 0.4421493709087372, 0.4113221764564514, 0.4237368404865265, 0.4607899785041809, 0.4399532973766327, 0.4595975875854492, 0.4386219382286072, 0.443441241979599, 0.43663454055786133, 0.4185038208961487, 0.40709975361824036, 0.42682793736457825, 0.47106611728668213, 0.445809006690979, 0.44649896025657654]\n",
      "\n",
      "Sentence = All successes begin with self-discipline . It starts with you .\n",
      "Emphasis Values = [0.42267996072769165, 0.39895597100257874, 0.3467350900173187, 0.38752636313438416, 0.4122341076533, 0.43057355284690857, 0.3821165859699249, 0.390667587518692, 0.3790017068386078, 0.3986576199531555, 0.42383304238319397]\n",
      "\n",
      "Sentence = It 's not a question of learning much . On the contrary . It 's a question of unlearning much .\n",
      "Emphasis Values = [0.4593207538127899, 0.5126418471336365, 0.4907241761684418, 0.4690489172935486, 0.492133766412735, 0.4679761826992035, 0.44411155581474304, 0.4346064329147339, 0.46399396657943726, 0.44340524077415466, 0.5077716708183289, 0.3929119110107422, 0.4796272814273834, 0.4693773686885834, 0.513079509139061, 0.4861537218093872, 0.5043179988861084, 0.5136546492576599, 0.47139329463243484, 0.4957974851131439, 0.46594223380088806]\n",
      "\n",
      "Sentence = When I count my blessings , I count you twice .\n",
      "Emphasis Values = [0.5271866917610168, 0.519280195236206, 0.5367550849914551, 0.5400689244270325, 0.4687260389328003, 0.5352539420127869, 0.5219663977622986, 0.5650873780250549, 0.4870114028453827, 0.5299530029296875, 0.5267013907432556]\n",
      "\n",
      "Sentence = Feel the fear and do it anyway .\n",
      "Emphasis Values = [0.5456564426422119, 0.5219709873199463, 0.5505288243293762, 0.5255864858627319, 0.5084437131881714, 0.4814645051956177, 0.48691773414611816, 0.5476990342140198]\n",
      "\n",
      "Sentence = A warm smile is the universal language of kindness .\n",
      "Emphasis Values = [0.4801952838897705, 0.4663602411746979, 0.45014622807502747, 0.45024925470352173, 0.46313270926475525, 0.46632206439971924, 0.4805908799171448, 0.4556598663330078, 0.4428166151046753, 0.47870680689811707]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicted Test Probablities\n",
    "for i in range(10):\n",
    "    print(\"Sentence = {}\".format(tokenized_test_text[i]))\n",
    "    print(\"Emphasis Values = {}\\n\".format(test_prob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "R2Aut3gpklGI"
   },
   "outputs": [],
   "source": [
    "#Dev Data\n",
    "#Done by Mithilaesh and Srihasa\n",
    "tokenized_dev_text = []\n",
    "for i in devWords:\n",
    "    sent = \"\"\n",
    "    for j in i:\n",
    "        if sent == \"\":\n",
    "            sent += j\n",
    "        else:\n",
    "            sent = sent + \" \" + j\n",
    "    tokenized_dev_text.append(sent)\n",
    "dev_features = feature_add(devWords, conn_words)\n",
    "dev_feature = feature_list(devWords,dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the model on Dev Dataset\n",
    "#Done by Mithilaesh and Srihasa\n",
    "dev_prob=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "      for batch_data, feat in zip(tokenized_dev_text, dev_feature):\n",
    "        cp = []\n",
    "        tokens = tokenizer.tokenize(batch_data)\n",
    "        tid = tokenizer.encode_plus(tokens, add_special_tokens=False, return_attention_mask=True, return_tensors='pt')\n",
    "        cp.append(feat)\n",
    "        dev_probas = model(tid['input_ids'], tid['attention_mask'], torch.tensor(cp))\n",
    "        dev_probas=dev_probas.data\n",
    "        out = batch_data.split(\" \")\n",
    "        temp_ans = []\n",
    "        index = 0\n",
    "        for i in out:\n",
    "            if (len(tokenizer.tokenize(i))) == 1:\n",
    "                temp_ans.append(dev_probas[0][index].item())\n",
    "                index = index + 1\n",
    "            else:\n",
    "                holder = []\n",
    "                for j in range(len(tokenizer.tokenize(i))):\n",
    "                    holder.append(dev_probas[0][index].item())\n",
    "                    index = index + 1\n",
    "                prb = np.average(holder)\n",
    "                temp_ans.append(prb) \n",
    "        dev_prob.append(temp_ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = Life is defined more by its risks than by its samenesses .\n",
      "Emphasis Values = [0.4668006896972656, 0.40153881907463074, 0.4603216052055359, 0.4468671381473541, 0.436434268951416, 0.42450734972953796, 0.448442280292511, 0.4330403506755829, 0.4563688039779663, 0.4017581045627594, 0.4246480464935303, 0.449434757232666]\n",
      "Ground Truth = [0.4444444444444444, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 1.0, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.7777777777777778, 0.1111111111111111]\n",
      "\n",
      "Sentence = There is magic in the night when pumpkins glow by moonlight .\n",
      "Emphasis Values = [0.4742286205291748, 0.39304935932159424, 0.49770572781562805, 0.4703209102153778, 0.47929349541664124, 0.4939355254173279, 0.501082718372345, 0.360744908452034, 0.471821665763855, 0.4787140190601349, 0.43171289563179016, 0.4839663505554199]\n",
      "Ground Truth = [0.2222222222222222, 0.2222222222222222, 0.8888888888888888, 0.3333333333333333, 0.3333333333333333, 0.4444444444444444, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111]\n",
      "\n",
      "Sentence = Think of all the beauty still left around you and be happy .\n",
      "Emphasis Values = [0.4771285951137543, 0.35046839714050293, 0.3383001983165741, 0.43947336077690125, 0.464684396982193, 0.33971717953681946, 0.47490963339805603, 0.4348186254501343, 0.45070213079452515, 0.47843486070632935, 0.4694025218486786, 0.4239670932292938, 0.4760702848434448]\n",
      "Ground Truth = [0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.7777777777777778, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.0, 0.2222222222222222, 0.8888888888888888, 0.3333333333333333]\n",
      "\n",
      "Sentence = Exploration is wired into our brains . If we can see the horizon , we want to know what 's beyond .\n",
      "Emphasis Values = [0.46247538924217224, 0.47020405530929565, 0.4680350422859192, 0.4141828715801239, 0.3908933699131012, 0.43304821848869324, 0.4875050187110901, 0.485444039106369, 0.4224108159542084, 0.46696189045906067, 0.5128849148750305, 0.4680945873260498, 0.4479573965072632, 0.4487186372280121, 0.396694153547287, 0.4745696485042572, 0.43923676013946533, 0.47063639760017395, 0.48699209094047546, 0.47635501623153687, 0.41960349678993225, 0.4876754581928253]\n",
      "Ground Truth = [0.6666666666666666, 0.1111111111111111, 0.3333333333333333, 0.2222222222222222, 0.2222222222222222, 0.5555555555555556, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.2222222222222222, 0.2222222222222222, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.4444444444444444, 0.1111111111111111]\n",
      "\n",
      "Sentence = At the end of the day we can endure much more than we think we can .\n",
      "Emphasis Values = [0.39473316073417664, 0.38163837790489197, 0.39930853247642517, 0.4251291751861572, 0.42837944626808167, 0.40304484963417053, 0.4631956219673157, 0.4262098968029022, 0.41920143365859985, 0.4583092927932739, 0.4396008849143982, 0.4363641142845154, 0.39125266671180725, 0.4964257776737213, 0.41766542196273804, 0.412111759185791, 0.47469303011894226]\n",
      "Ground Truth = [0.1111111111111111, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.2222222222222222, 0.4444444444444444, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.5555555555555556, 0.1111111111111111, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111]\n",
      "\n",
      "Sentence = Be kind to unkind people , they need it the most .\n",
      "Emphasis Values = [0.48854655027389526, 0.4987195134162903, 0.42738959193229675, 0.43582913279533386, 0.4487144947052002, 0.43690621852874756, 0.44088515639305115, 0.46018362045288086, 0.43095454573631287, 0.44498151540756226, 0.42719757556915283, 0.5118905901908875]\n",
      "Ground Truth = [0.4444444444444444, 0.7777777777777778, 0.1111111111111111, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.4444444444444444, 0.1111111111111111, 0.1111111111111111, 0.5555555555555556, 0.1111111111111111]\n",
      "\n",
      "Sentence = She believed she could , so she did .\n",
      "Emphasis Values = [0.5119898319244385, 0.5196571946144104, 0.40763524174690247, 0.4819008409976959, 0.4215560853481293, 0.4719606041908264, 0.42870938777923584, 0.4871309995651245, 0.4772062301635742]\n",
      "Ground Truth = [0.2222222222222222, 0.7777777777777778, 0.1111111111111111, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.3333333333333333]\n",
      "\n",
      "Sentence = Tick Tock Our 24 Hr Sale Starts Now !\n",
      "Emphasis Values = [0.5481414794921875, 0.4904068261384964, 0.5118584632873535, 0.522612452507019, 0.48998746275901794, 0.5311583876609802, 0.543857753276825, 0.49876850843429565, 0.5226941704750061]\n",
      "Ground Truth = [0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.0, 0.5555555555555556, 0.1111111111111111]\n",
      "\n",
      "Sentence = Holly Jolly Holiday Party\n",
      "Emphasis Values = [0.5642539858818054, 0.5401402115821838, 0.5504048466682434, 0.464984267950058]\n",
      "Ground Truth = [0.1111111111111111, 0.2222222222222222, 0.6666666666666666, 0.7777777777777778]\n",
      "\n",
      "Sentence = April showers bring May flowers\n",
      "Emphasis Values = [0.41391047835350037, 0.3864949345588684, 0.4326384961605072, 0.4087952673435211, 0.4519540071487427]\n",
      "Ground Truth = [0.7777777777777778, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.7777777777777778]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicted Dev Probablities\n",
    "for i in range(10):\n",
    "    print(\"Sentence = {}\".format(tokenized_dev_text[i]))\n",
    "    print(\"Emphasis Values = {}\".format(dev_prob[i]))\n",
    "    print(\"Ground Truth = {}\\n\".format(devLabels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting first 4 emphasized words\n",
    "#Done by Phani\n",
    "def finalProbs(data,values):        \n",
    "    temp_list = [list(x) for x in zip(data,values)]\n",
    "    sentence_list = []\n",
    "    probas_list = []\n",
    "    for sentences,probas in temp_list:\n",
    "        sentence_list.append([[list] for list in sentences])\n",
    "        probas_list.append([prob for prob in probas])\n",
    "\n",
    "    wordsFinal = []\n",
    "    probFinal = []\n",
    "    temp2 = []\n",
    "    for word, prob in zip(sentence_list,probas_list):\n",
    "        wordList = []\n",
    "        probList = []\n",
    "        for i,j in zip(word,prob):\n",
    "            if not(i[0].startswith(\"##\")):\n",
    "                wordList.append(i)\n",
    "                probList.append(j)\n",
    "            else:\n",
    "                wordTemp = wordList[-1]+[i[0]]\n",
    "                probTemp = probList[-1]+[j[0]]\n",
    "                wordTemp = [''.join(wordTemp)]\n",
    "                wordList.append(wordTemp)\n",
    "                probList.append(probTemp)\n",
    "                del(wordList[-2])\n",
    "                del(probList[-2])\n",
    "      \n",
    "        for k in probList:\n",
    "            if len(k) == 1:\n",
    "                temp2.append(k)\n",
    "            else:\n",
    "                average = [np.average(k)]\n",
    "                temp2.append(average)\n",
    "        wordsFinal.append(wordList)\n",
    "        probFinal.append(temp2)\n",
    "        wordList = []\n",
    "        probList = []\n",
    "        temp2 = []\n",
    "    return wordsFinal,probFinal\n",
    "\n",
    "def compute_loss(i):\n",
    "    wlist = []\n",
    "    plist = []\n",
    "    for j in i:\n",
    "        wlist.append(j[0])\n",
    "        plist.append(j[1])\n",
    "        wtemp = []\n",
    "        ptemp = []\n",
    "    for i,j in sorted(zip(plist,wlist),reverse = True):\n",
    "        wtemp.append(j)\n",
    "        ptemp.append(i)\n",
    "        \n",
    "    wfinal = []\n",
    "    loss = []\n",
    "    finalList = []\n",
    "    for i,j in zip(wtemp,ptemp):\n",
    "        for k,l in zip(wtemp[1:],ptemp[1:]):\n",
    "            currentWord = i[0]\n",
    "            currentProb = float(j[0])\n",
    "            nextprob = float(l[0])\n",
    "            temp = currentProb - nextprob\n",
    "            lossTemp = -max((temp),0) * math.log1p(temp)\n",
    "            loss.append(lossTemp)\n",
    "        wfinal.append([[currentWord],[currentProb],[np.average(loss)]])\n",
    "    finalList.append(wfinal)\n",
    "    \n",
    "    return finalList\n",
    "\n",
    "def final_rank(words,probs):\n",
    "    loss_test = [] \n",
    "    for i,j in zip(words,probs):\n",
    "        loss_temp = []\n",
    "        for k,l in zip(i,j):\n",
    "            if '##' in k[0]:\n",
    "                loss_temp.append([k,l])\n",
    "        if loss_temp is []:\n",
    "            loss_temp.append('[]')\n",
    "        loss_test.append(loss_temp)\n",
    "    \n",
    "    \n",
    "    subword_dict = []\n",
    "    subword_list = []\n",
    "    for i in loss_test:\n",
    "        empty_dict = dict.fromkeys(['Rank1','Rank2','Rank3','Rank4'])\n",
    "        if (i == []):\n",
    "            subword_list.append([[\"No subwords\"]])\n",
    "            subword_dict.append(empty_dict)\n",
    "            continue\n",
    "        else:\n",
    "            if (len(i) == 1):\n",
    "                subword_list.append(i)\n",
    "                for a in i:\n",
    "                    empty_dict['Rank1'] = a[0]\n",
    "                subword_dict.append(empty_dict)\n",
    "            else:\n",
    "                j = compute_loss(i)\n",
    "                subword_list.append(j)\n",
    "                for c in j:\n",
    "                    for d in c:\n",
    "                        if empty_dict['Rank1'] is None:\n",
    "                            empty_dict['Rank1'] = d[0]\n",
    "                        elif empty_dict['Rank2'] is None:\n",
    "                             empty_dict['Rank2'] = d[0]\n",
    "                        elif empty_dict['Rank3'] is None:\n",
    "                            empty_dict['Rank3'] = d[0]\n",
    "                        else:\n",
    "                            empty_dict['Rank4'] = d[0]\n",
    "                subword_dict.append(empty_dict)\n",
    "                \n",
    "    wd = []\n",
    "    for i,j in zip(testWords,testProbs):\n",
    "        dic = sorted(zip(j,i),reverse=True)\n",
    "        wd.append(dic)\n",
    "        word_dict = []    \n",
    "    for i,k in zip(wd,subword_dict):\n",
    "        empty_word_dict = dict.fromkeys(['Rank1','Rank2','Rank3','Rank4'])\n",
    "        for j in i:\n",
    "            if '##' not in j[1][0]:\n",
    "                if empty_word_dict['Rank1'] is None:\n",
    "                    empty_word_dict['Rank1'] = j[1]\n",
    "                elif empty_word_dict['Rank2'] is None:\n",
    "                    empty_word_dict['Rank2'] = j[1]\n",
    "                elif empty_word_dict['Rank3'] is None:\n",
    "                    empty_word_dict['Rank3'] = j[1]\n",
    "                elif empty_word_dict['Rank4'] is None:\n",
    "                    empty_word_dict['Rank4'] = j[1]\n",
    "        word_dict.append(empty_word_dict)\n",
    "    \n",
    "    \n",
    "    final_word_dict = []\n",
    "    for i,j in zip(subword_dict,word_dict):\n",
    "        final_word_dict.append((i,j))\n",
    "    return final_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running pairwise ranking for getting top 4 words\n",
    "wordList = []\n",
    "probList = []\n",
    "test_res = [item for sublist in test_res for item in sublist]\n",
    "testWords,testProbs = finalProbs(test_tokens,test_res)\n",
    "finalDict = final_rank(testWords,testProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rank1': ['/'], 'Rank2': ['!'], 'Rank3': ['see'], 'Rank4': ['/']}\n",
      "{'Rank1': ['.'], 'Rank2': ['how'], 'Rank3': ['matter'], 'Rank4': ['harder']}\n",
      "{'Rank1': ['less'], 'Rank2': ['felt'], 'Rank3': ['better'], 'Rank4': ['the']}\n",
      "{'Rank1': ['believe'], 'Rank2': ['who'], 'Rank3': ['.'], 'Rank4': ['time']}\n",
      "{'Rank1': ['you'], 'Rank2': ['a'], 'Rank3': ['the'], 'Rank4': ['not']}\n",
      "{'Rank1': ['-'], 'Rank2': ['.'], 'Rank3': ['.'], 'Rank4': ['all']}\n",
      "{'Rank1': ['un##lea##rn##ing'], 'Rank2': None, 'Rank3': None, 'Rank4': None}\n",
      "{'Rank1': ['count'], 'Rank2': ['my'], 'Rank3': ['count'], 'Rank4': [',']}\n",
      "{'Rank1': ['fear'], 'Rank2': ['.'], 'Rank3': ['feel'], 'Rank4': ['and']}\n",
      "{'Rank1': ['language'], 'Rank2': ['a'], 'Rank3': ['.'], 'Rank4': ['warm']}\n",
      "{'Rank1': ['er##r'], 'Rank2': None, 'Rank3': None, 'Rank4': None}\n",
      "{'Rank1': ['.'], 'Rank2': ['making'], 'Rank3': [','], 'Rank4': ['not']}\n",
      "{'Rank1': ['matt##ie'], 'Rank2': None, 'Rank3': None, 'Rank4': None}\n",
      "{'Rank1': ['year'], 'Rank2': ['lunar'], 'Rank3': ['happy'], 'Rank4': ['new']}\n",
      "{'Rank1': ['magnolia'], 'Rank2': ['theatre'], 'Rank3': None, 'Rank4': None}\n",
      "{'Rank1': ['-'], 'Rank2': ['mid'], 'Rank3': ['happy'], 'Rank4': ['festival']}\n",
      "{'Rank1': ['prepare'], 'Rank2': ['something'], 'Rank3': ['job'], 'Rank4': ['.']}\n",
      "{'Rank1': ['mu##ssel##s'], 'Rank2': None, 'Rank3': None, 'Rank4': None}\n",
      "{'Rank1': ['create'], 'Rank2': ['is'], 'Rank3': ['predict'], 'Rank4': ['the']}\n",
      "{'Rank1': ['a##€##¢'], 'Rank2': ['a##€##¢'], 'Rank3': ['ari##es'], 'Rank4': None}\n"
     ]
    }
   ],
   "source": [
    "#printing the values got through pairwise ranking\n",
    "#Done by Mithilaesh\n",
    "for i in finalDict[:20]:\n",
    "    if (i[0]).get('Rank1') is None:\n",
    "        print(i[1])\n",
    "    else:\n",
    "        print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmphasisSelection_Updated.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
